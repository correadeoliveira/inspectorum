<p align="center">
  <img src="frontend/public/logo.png" alt="Logo do Inspectorum" width="150">
</p>

# Inspectorum: Exame de Consci√™ncia com IA

Inspectorum √© uma aplica√ß√£o de desktop local e privada para a pr√°tica do exame de consci√™ncia cat√≥lico. A ferramenta guia o usu√°rio atrav√©s de uma s√©rie de perguntas baseadas nos Dez Mandamentos, permitindo uma reflex√£o estruturada. Ao final, utiliza modelos de linguagem locais (LLMs) para gerar uma an√°lise espiritual personalizada sobre os pontos que necessitam de mais aten√ß√£o, al√©m de alimentar um painel de progresso para acompanhamento di√°rio e semanal.

O projeto tamb√©m inclui um assistente de doutrina que responde a perguntas do usu√°rio com base em uma cole√ß√£o de documentos fornecidos (RAG), garantindo respostas fi√©is e contextualizadas.

Toda a opera√ß√£o ocorre localmente na m√°quina do usu√°rio, garantindo 100% de privacidade.

## ‚ú® Funcionalidades

* **Exame de Consci√™ncia Guiado:** Um chatbot que apresenta perguntas de forma sequencial.
* **An√°lise por IA:** Ap√≥s o exame, uma IA local analisa as respostas e gera uma reflex√£o pastoral.
* **Painel de Progresso:** Gr√°ficos e estat√≠sticas que monitoram a evolu√ß√£o espiritual do usu√°rio.
* **Assistente de Doutrina (RAG):** Um modo de chat p√≥s-an√°lise que permite ao usu√°rio tirar d√∫vidas sobre a f√©.
* **Privacidade Total:** Todos os dados, LLMs e processos rodam localmente.

## üõ†Ô∏è Stack Tecnol√≥gica

* **Frontend:** Next.js (React), TypeScript, Tailwind CSS, shadcn/ui.
* **Backend:** Python, Flask, LangChain.
* **Modelos de Linguagem:** Ollama (rodando `phi3:3.8b-mini-4k-instruct-q4_0` localmente).
* **Banco de Dados Vetorial:** FAISS para o sistema RAG.
* **Banco de Dados de Progresso:** SQLite.
* **Orquestrador:** Node.js, pnpm e `concurrently`.

## üöÄ Guia de Instala√ß√£o (Do Zero)

Siga estes passos para configurar o projeto em uma nova m√°quina.

### 1. Pr√©-requisitos

Certifique-se de que os seguintes softwares est√£o instalados:

* **Git:** Para clonar o reposit√≥rio.
* **Python:** Vers√£o 3.9 ou superior.
* **Node.js:** Vers√£o 18 ou superior.
* **pnpm:** Ap√≥s instalar o Node.js, instale o pnpm globalmente:
    ```bash
    npm install -g pnpm
    ```
* **Ollama:** Baixe e instale o aplicativo do [Ollama](https://ollama.com/). Ap√≥s a instala√ß√£o, **inicie o aplicativo** para que ele fique rodando em segundo plano.

### 2. Instala√ß√£o do Projeto

1.  **Clone o Reposit√≥rio:**
    ```bash
    git clone <URL_DO_SEU_REPOSITORIO>
    cd projeto-exame-consciencia
    ```

2.  **Instale as Depend√™ncias do Backend (Python):**
    ```bash
    # Entre na pasta do backend
    cd backend
    # Crie o ambiente virtual
    python -m venv venv
    # Ative o ambiente (use o comando para o seu sistema operacional)
    # No Windows:
    .\venv\Scripts\activate
    # No macOS/Linux:
    source venv/bin/activate
    # Instale os pacotes
    pip install -r requirements.txt
    # Volte para a raiz do projeto
    cd ..
    ```

3.  **Instale as Depend√™ncias do Frontend (Node.js):**
    ```bash
    # Na pasta raiz, instale as depend√™ncias da raiz
    pnpm install
    # Instale as depend√™ncias do subprojeto frontend
    pnpm --prefix frontend install
    ```

4.  **Configure o Conte√∫do (RAG):**
    * Dentro da pasta `backend/`, localize a subpasta `documentos/`.
    * **Coloque seus arquivos PDF** de refer√™ncia dentro desta pasta. Sem eles, o "Assistente de Doutrina" n√£o ter√° base de conhecimento.

5.  **Execute o Script de Setup Automatizado:**
    * Este script ir√° criar o banco de dados, construir o √≠ndice dos seus PDFs e baixar o modelo de IA.
    * Certifique-se de que o ambiente virtual do backend (`venv`) esteja ativo.
    * Na pasta **raiz** do projeto, execute:
        ```bash
        python backend/setup.py
        ```

### 3. Como Rodar a Aplica√ß√£o (Uso Di√°rio)

Ap√≥s a configura√ß√£o inicial ser conclu√≠da, o processo para iniciar o aplicativo √© muito simples:

1.  **Verifique se o Ollama est√° rodando** em segundo plano (√≠cone na barra de tarefas).
2.  Abra um terminal na **pasta raiz** do projeto.
3.  **Ative o ambiente virtual do backend**:
    * **No Windows:** `.\backend\venv\Scripts\activate`
    * **No macOS/Linux:** `source backend/venv/bin/activate`
4.  Execute o comando principal:
    ```bash
    pnpm run dev
    ```
O backend e o frontend ser√£o iniciados, e seu navegador abrir√° automaticamente em `http://localhost:3000`.

## ‚öôÔ∏è Funcionamento T√©cnico

* **Backend (Flask):** Serve uma API local que gerencia o estado do exame, executa as duas an√°lises de LLM (a textual e a de classifica√ß√£o), salva os dados no SQLite e processa as perguntas para o sistema RAG.
* **Frontend (Next.js):** Constr√≥i a interface do usu√°rio, incluindo o chatbot e o dashboard, e se comunica com a API Flask para buscar e enviar dados.
* **Fluxo de Dados:** O exame come√ßa lendo o `perguntas.yaml`. As respostas s√£o salvas em `respostas.yaml`. A an√°lise de progresso √© salva em `progress.db`, que por sua vez alimenta o dashboard. O chat RAG consulta o √≠ndice `faiss_index_mistral` para responder √†s perguntas.
  
## ü§ù Como Contribuir

Consulte o nosso guia de contribui√ß√£o em [`CONTRIBUTING.md`](./CONTRIBUTING.md) para saber como voc√™ pode ajudar a melhorar o Inspectorum.

## üìÑ Licen√ßa

Este projeto √© licenciado sob a Licen√ßa MIT. Veja o arquivo [`LICENSE`](./LICENSE) para mais detalhes.
